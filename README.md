# Child and Adult Speech Classification
The aim of this project is to develop an algorithm capable of muting audio recordings when adults are speaking, without muting baby noises such as crying, coughing, laughing, babbling, etc. I will be experimenting on Random forest classifier, neural networks and CNN + RNN combination.

## Installation
Clone the repository into your local machine
```
git clone https://github.com/katabattunisuma/Child-and-Adult-Speech-Classification.git
```
Set up [virtual environment](https://docs.python.org/3/library/venv.html) if needed and then install requirements.txt using the following command.
```
pip install requirements.txt
```

### Dataset
I have taken child dataset from donate-a-cry-corpus-features-dataset and adult dataset from Kaggle. These files can be found in Data folder. 

## Feature Extraction
In this project I have tried 3 different featre extractors which can be found in feature_extraction.py
### 1. librosa.feature.mfcc
A function in the Librosa library, utilized for extracting Mel-frequency cepstral coefficients (MFCCs) from audio signals. MFCCs are a widely-used feature in audio analysis, particularly valuable in speech and music processing tasks. They provide a compact representation of the power spectrum of an audio signal, based on a linear cosine transform of a log power spectrum on a nonlinear Mel scale. This method is instrumental in various applications like speech recognition, music genre classification, and audio similarity analysis.

### 2. librosa.feature.melspectrogram
This function in the Librosa library is designed for computing the Mel-scaled spectrogram from an audio signal. A Mel-spectrogram is an advanced representation that portrays the power spectrum of sound frequencies over time, scaled to the Mel scale. This scaling is pivotal because it reflects the human ear's nonlinear perception of sound, giving more emphasis to lower frequencies. The Mel-spectrogram is a crucial feature in various audio analysis tasks, including speech recognition, music genre classification, and environmental sound analysis, providing rich insights into the spectral content of audio data.

### 3. librosa.feature.mfcc along with Spectral components

The extract_feature function is a comprehensive audio feature extraction tool designed to analyze audio data and extract a variety of meaningful features. This function is essential for audio analysis tasks such as speech recognition, music classification, and environmental sound analysis. Here’s a breakdown of the features it extracts and their significance:

+ _**Zero Crossing Rate (ZCR)**_: Measures the rate at which the audio signal changes sign. ZCR is useful for identifying percussive elements and can be indicative of the rhythm or the noisiness of the audio.

+ _**Mel-frequency Cepstral Coefficients (MFCCs)**_: Thirteen MFCCs are extracted, providing a compact representation of the power spectrum. MFCCs are crucial in voice recognition and music analysis, capturing timbral and harmonic characteristics.

+ _**Spectral Centroid:**_ Represents the 'center of mass' of the spectrum, indicating the brightness of a sound. It’s useful for distinguishing between different types of sounds and understanding their tonal quality.

+ _**Spectral Bandwidth:**_ Measures the width of the spectral energy distribution, giving an idea of the sound’s textural richness.

+ _**Spectral Rolloff:**_ Indicates the frequency below which a specified percentage of the total spectral energy (90% in this case) is contained. It helps in distinguishing between harmonic and noisy sounds.

+ _**Spectral Flatness:**_ Measures how noise-like a sound is, as opposed to being tonal. A higher flatness indicates a more noise-like sound and vice versa.

## Model Architecture 
<img width="790" alt="Screenshot 2023-12-03 at 3 41 58 PM" src="https://github.com/katabattunisuma/Child-and-Adult-Speech-Classification/assets/51089800/be12de34-1bc9-4b5f-aa1a-a20f9c98b1c2">


## Test Data Preparation
Code related to this section can be found in Combined_data.py
#### Overview:
Generates 20-second audio files by randomly selecting 2-second segments from baby and adult voice recordings, along with corresponding labels.

#### Parameters:

total_audios: Number of generated audio files (default: 10,000).
output_audio_length: Desired length of output audio (default: 20 seconds).
segment_sizes: List of segment sizes for random selection (default: [1500, 2000, 2500, 3000]).

## Testing pipeline
Once the model is trained for testing purposes created a pipeline for machine learning model (Tests/test_ml_model.py) and neural networks (Tests/test_neural_network.py) which use the data generated by combined_audio.py
